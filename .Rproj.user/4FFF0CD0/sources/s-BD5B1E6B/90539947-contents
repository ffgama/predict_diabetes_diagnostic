# teste local do modelo

# Etapa de criação do modelo
rm(list=ls())

# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)

library(beepr)
library(mlr)

# tamanho dos dados para treino
size_sample <-  floor(0.7 * nrow(data_select))

set.seed(123)
# particionamento aleatório
train_ind <- sample(seq_len(nrow(data_select)), size_sample)

# training x test
data_train <- data_select[train_ind, ]
data_test <- data_select[-train_ind, ]

# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
test_task <- makeClassifTask(data = data_test, target = "classe")

# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
test_task <- normalizeFeatures(test_task, method = "standardize")

im_feat <- generateFilterValuesData(train_task, method = c("information.gain","chi.squared"))
#plotFilterValues(im_feat,n.show = 20)

# removendo features 
train_task <- dropFeatures(task = train_task,features = c("grossura_pele"))

rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 2, ntree = 200, importance = TRUE))

getParamSet("classif.randomForest")

# gridsearch (parametros de tuning)
parameters <- makeParamSet(
  makeIntegerParam("ntree",lower = 50, upper = 500),
  makeIntegerParam("mtry", lower = 2, upper = 7),
  makeIntegerParam("nodesize", lower = 10, upper = 50)
)

control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)

rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)

# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)

# configurando os parâmetros de tuning 
rf <- mlr::train(rf_tuning, train_task)
getLearnerModel(rf)

# passando o conjunto de teste para o modelo 
predict_rf<-predict(rf, test_task)
predict_rf

df <- data.frame(predict_rf$data)

# definindo a classe positiva como 1 (com diabetes)
# avaliar os resultados
library(caret)
confusionMatrix(df$response, df$truth, positive = levels(df$truth)[2])

# avaliando onde o modelo está errando 
# df_error <- df[df$truth != df$response, ]
# df_error
# 
# # selecionar as instâncias que o modelo errou
# data_select_error <- data_select[df_error$id,]
# data_select_error
beep()
