head(data_select)
# library(ROSE)
#
# método de over e undersampling
# balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "over",N = 680, seed = 1)$data
# data_select <- balanced_target
data_select$classe <- as.factor(data_select$classe)
library(DMwR)
table(data_select$classe)
balanced_target <- SMOTE(classe ~., data = data_select,  perc.over = 70, perc.under = 180)
as.data.frame(table(balanced_target$classe))
# teste local do modelo
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
# library(ROSE)
#
# método de over e undersampling
# balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "over",N = 680, seed = 1)$data
# data_select <- balanced_target
data_select$classe <- as.factor(data_select$classe)
library(DMwR)
table(data_select$classe)
balanced_target <- SMOTE(classe ~., data = data_select,  perc.over = 70, perc.under = 180)
as.data.frame(table(balanced_target$classe))
# teste local do modelo
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
# library(ROSE)
#
# método de over e undersampling
# balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "over",N = 680, seed = 1)$data
# data_select <- balanced_target
data_select$classe <- as.factor(data_select$classe)
library(DMwR)
table(data_select$classe)
balanced_target <- SMOTE(classe ~., data = data_select,  perc.over = 70, perc.under = 180)
as.data.frame(table(balanced_target$classe))
# teste local do modelo
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
# library(ROSE)
#
# método de over e undersampling
# balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "over",N = 680, seed = 1)$data
# data_select <- balanced_target
data_select$classe <- as.factor(data_select$classe)
library(DMwR)
table(data_select$classe)
balanced_target <- SMOTE(classe ~., data = data_select,  perc.over = 70, perc.under = 180)
# as.data.frame(table(balanced_target$classe))
data_select <- balanced_target
library(beepr)
library(mlr)
# tamanho dos dados para treino
size_sample <-  floor(0.7 * nrow(data_select))
set.seed(123)
# particionamento aleatório
train_ind <- sample(seq_len(nrow(data_select)), size_sample)
# training x test
data_train <- data_select[train_ind, ]
data_test <- data_select[-train_ind, ]
# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
test_task <- makeClassifTask(data = data_test, target = "classe")
# # normalizando as variáveis
# train_task <- normalizeFeatures(train_task, method = "standardize" )
# test_task <- normalizeFeatures(test_task, method = "standardize")
im_feat <- generateFilterValuesData(train_task, method = c("information.gain","chi.squared"))
#plotFilterValues(im_feat,n.show = 20)
# removendo features
train_task <- dropFeatures(task = train_task,features = c("grossura_pele"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 100, importance = TRUE))
getParamSet("classif.randomForest")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 5, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
getLearnerModel(rf)
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf, test_task)
predict_rf
df <- data.frame(predict_rf$data)
# definindo a classe positiva como 1 (com diabetes)
# avaliar os resultados
library(caret)
confusionMatrix(df$response, df$truth, positive = levels(df$truth)[2])
# avaliando onde o modelo está errando
# df_error <- df[df$truth != df$response, ]
# df_error
#
# # selecionar as instâncias que o modelo errou
# data_select_error <- data_select[df_error$id,]
# data_select_error
beep()
roc.curve(df$truth, df$response, plotit = F)
# teste local do modelo
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
library(ROSE)
#
# método de over e undersampling
balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "over",N = 680, seed = 1)$data
data_select <- balanced_target
# data_select$classe <- as.factor(data_select$classe)
#
# library(DMwR)
#
# table(data_select$classe)
#
# balanced_target <- SMOTE(classe ~., data = data_select,  perc.over = 70, perc.under = 180)
# as.data.frame(table(balanced_target$classe))
data_select <- balanced_target
library(beepr)
library(mlr)
# tamanho dos dados para treino
size_sample <-  floor(0.7 * nrow(data_select))
set.seed(123)
# particionamento aleatório
train_ind <- sample(seq_len(nrow(data_select)), size_sample)
# training x test
data_train <- data_select[train_ind, ]
data_test <- data_select[-train_ind, ]
# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
test_task <- makeClassifTask(data = data_test, target = "classe")
# # normalizando as variáveis
# train_task <- normalizeFeatures(train_task, method = "standardize" )
# test_task <- normalizeFeatures(test_task, method = "standardize")
im_feat <- generateFilterValuesData(train_task, method = c("information.gain","chi.squared"))
#plotFilterValues(im_feat,n.show = 20)
# removendo features
train_task <- dropFeatures(task = train_task,features = c("grossura_pele"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 100, importance = TRUE))
getParamSet("classif.randomForest")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 5, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
getLearnerModel(rf)
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf, test_task)
predict_rf
df <- data.frame(predict_rf$data)
# definindo a classe positiva como 1 (com diabetes)
# avaliar os resultados
library(caret)
confusionMatrix(df$response, df$truth, positive = levels(df$truth)[2])
# avaliando onde o modelo está errando
# df_error <- df[df$truth != df$response, ]
# df_error
#
# # selecionar as instâncias que o modelo errou
# data_select_error <- data_select[df_error$id,]
# data_select_error
beep()
roc.curve(df$truth, df$response, plotit = F)
# teste local do modelo
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
library(ROSE)
#
# método de over e undersampling
balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "both",N = 680, seed = 1)$data
data_select <- balanced_target
# data_select$classe <- as.factor(data_select$classe)
#
# library(DMwR)
#
# table(data_select$classe)
#
# balanced_target <- SMOTE(classe ~., data = data_select,  perc.over = 70, perc.under = 180)
# as.data.frame(table(balanced_target$classe))
data_select <- balanced_target
library(beepr)
library(mlr)
# tamanho dos dados para treino
size_sample <-  floor(0.7 * nrow(data_select))
set.seed(123)
# particionamento aleatório
train_ind <- sample(seq_len(nrow(data_select)), size_sample)
# training x test
data_train <- data_select[train_ind, ]
data_test <- data_select[-train_ind, ]
# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
test_task <- makeClassifTask(data = data_test, target = "classe")
# # normalizando as variáveis
# train_task <- normalizeFeatures(train_task, method = "standardize" )
# test_task <- normalizeFeatures(test_task, method = "standardize")
im_feat <- generateFilterValuesData(train_task, method = c("information.gain","chi.squared"))
#plotFilterValues(im_feat,n.show = 20)
# removendo features
train_task <- dropFeatures(task = train_task,features = c("grossura_pele"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 100, importance = TRUE))
getParamSet("classif.randomForest")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 5, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
getLearnerModel(rf)
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf, test_task)
predict_rf
df <- data.frame(predict_rf$data)
# definindo a classe positiva como 1 (com diabetes)
# avaliar os resultados
library(caret)
confusionMatrix(df$response, df$truth, positive = levels(df$truth)[2])
# avaliando onde o modelo está errando
# df_error <- df[df$truth != df$response, ]
# df_error
#
# # selecionar as instâncias que o modelo errou
# data_select_error <- data_select[df_error$id,]
# data_select_error
beep()
roc.curve(df$truth, df$response, plotit = F)
# teste local do modelo
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
library(ROSE)
#
# método de over e undersampling
balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "both",N = 700, seed = 1)$data
data_select <- balanced_target
# data_select$classe <- as.factor(data_select$classe)
#
# library(DMwR)
#
# table(data_select$classe)
#
# balanced_target <- SMOTE(classe ~., data = data_select,  perc.over = 70, perc.under = 180)
# as.data.frame(table(balanced_target$classe))
data_select <- balanced_target
library(beepr)
library(mlr)
# tamanho dos dados para treino
size_sample <-  floor(0.7 * nrow(data_select))
set.seed(123)
# particionamento aleatório
train_ind <- sample(seq_len(nrow(data_select)), size_sample)
# training x test
data_train <- data_select[train_ind, ]
data_test <- data_select[-train_ind, ]
# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
test_task <- makeClassifTask(data = data_test, target = "classe")
# # normalizando as variáveis
# train_task <- normalizeFeatures(train_task, method = "standardize" )
# test_task <- normalizeFeatures(test_task, method = "standardize")
im_feat <- generateFilterValuesData(train_task, method = c("information.gain","chi.squared"))
#plotFilterValues(im_feat,n.show = 20)
# removendo features
train_task <- dropFeatures(task = train_task,features = c("grossura_pele"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 100, importance = TRUE))
getParamSet("classif.randomForest")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 5, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
getLearnerModel(rf)
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf, test_task)
predict_rf
df <- data.frame(predict_rf$data)
# definindo a classe positiva como 1 (com diabetes)
# avaliar os resultados
library(caret)
confusionMatrix(df$response, df$truth, positive = levels(df$truth)[2])
# avaliando onde o modelo está errando
# df_error <- df[df$truth != df$response, ]
# df_error
#
# # selecionar as instâncias que o modelo errou
# data_select_error <- data_select[df_error$id,]
# data_select_error
beep()
roc.curve(df$truth, df$response, plotit = F)
# teste local do modelo
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
library(ROSE)
#
# método de over e undersampling
balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "both",N = 680, seed = 1)$data
data_select <- balanced_target
# data_select$classe <- as.factor(data_select$classe)
#
# library(DMwR)
#
# table(data_select$classe)
#
# balanced_target <- SMOTE(classe ~., data = data_select,  perc.over = 70, perc.under = 180)
# as.data.frame(table(balanced_target$classe))
data_select <- balanced_target
library(beepr)
library(mlr)
# tamanho dos dados para treino
size_sample <-  floor(0.7 * nrow(data_select))
set.seed(123)
# particionamento aleatório
train_ind <- sample(seq_len(nrow(data_select)), size_sample)
# training x test
data_train <- data_select[train_ind, ]
data_test <- data_select[-train_ind, ]
# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
test_task <- makeClassifTask(data = data_test, target = "classe")
# # normalizando as variáveis
# train_task <- normalizeFeatures(train_task, method = "standardize" )
# test_task <- normalizeFeatures(test_task, method = "standardize")
im_feat <- generateFilterValuesData(train_task, method = c("information.gain","chi.squared"))
#plotFilterValues(im_feat,n.show = 20)
# removendo features
train_task <- dropFeatures(task = train_task,features = c("grossura_pele"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 100, importance = TRUE))
getParamSet("classif.randomForest")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 5, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
getLearnerModel(rf)
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf, test_task)
predict_rf
df <- data.frame(predict_rf$data)
# definindo a classe positiva como 1 (com diabetes)
# avaliar os resultados
library(caret)
confusionMatrix(df$response, df$truth, positive = levels(df$truth)[2])
# avaliando onde o modelo está errando
# df_error <- df[df$truth != df$response, ]
# df_error
#
# # selecionar as instâncias que o modelo errou
# data_select_error <- data_select[df_error$id,]
# data_select_error
beep()
roc.curve(df$truth, df$response, plotit = F)
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
library(ROSE)
# método de over e undersampling
balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "both",N = 680, seed = 1)$data
data_select <- balanced_target
############################# Preparando para a submissão ao Kaggle #############################
library(mlr)
set.seed(123)
# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
# # normalizando as variáveis
# train_task <- normalizeFeatures(train_task, method = "standardize" )
im_feat <- generateFilterValuesData(train_task, method = c("information.gain","chi.squared"))
#plotFilterValues(im_feat,n.show = 20)
# removendo features
train_task <- dropFeatures(task = train_task, features = c("grossura_pele"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 100, importance = TRUE))
getParamSet("classif.randomForest")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 5, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
# realizar as predições no dataset de teste do kaggle
data_test <- read.csv("data/dataset_teste.csv", header = TRUE, sep = ",")
# data_test <- normalizeFeatures(data_test, method = "standardize")
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf$learner.model, data_test)
predict_rf
# adequando para o formato do kaggle
df_predictions <- data.frame(id = cbind(1:length(predict_rf),as.data.frame(predict_rf)))
colnames(df_predictions) <- c("id", "classe")
head(df_predictions)
tail(df_predictions)
# gravando a saida
write.csv(df_predictions,file="submissoes/df_predictions_4.csv", row.names=FALSE)
# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
library(ROSE)
# método de over e undersampling
balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "both",N = 680, seed = 1)$data
data_select <- balanced_target
############################# Preparando para a submissão ao Kaggle #############################
library(mlr)
set.seed(123)
# criando as tasks
train_task <- makeClassifTask(data = data_select, target = "classe")
# # normalizando as variáveis
# train_task <- normalizeFeatures(train_task, method = "standardize" )
im_feat <- generateFilterValuesData(train_task, method = c("information.gain","chi.squared"))
#plotFilterValues(im_feat,n.show = 20)
# removendo features
train_task <- dropFeatures(task = train_task, features = c("grossura_pele"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 100, importance = TRUE))
getParamSet("classif.randomForest")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 5, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
# realizar as predições no dataset de teste do kaggle
data_test <- read.csv("data/dataset_teste.csv", header = TRUE, sep = ",")
# data_test <- normalizeFeatures(data_test, method = "standardize")
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf$learner.model, data_test)
predict_rf
# adequando para o formato do kaggle
df_predictions <- data.frame(id = cbind(1:length(predict_rf),as.data.frame(predict_rf)))
colnames(df_predictions) <- c("id", "classe")
head(df_predictions)
tail(df_predictions)
# gravando a saida
write.csv(df_predictions,file="submissoes/df_predictions_4.csv", row.names=FALSE)
source('~/GitHub/predict_diabetes_diagnostic/model.R', encoding = 'UTF-8')
source('~/GitHub/predict_diabetes_diagnostic/model.R', encoding = 'UTF-8')
