# df_error <- df[df$truth != df$response, ]
# df_error
#
# # selecionar as instâncias que o modelo errou
# data_select_error <- data_select[df_error$id,]
# data_select_error
beep()
roc.curve(df$truth, df$response, plotit = F)
# teste local do modelo
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
library(beepr)
library(mlr)
# tamanho dos dados para treino
size_sample <-  floor(0.7 * nrow(data_select))
set.seed(123)
# particionamento aleatório
train_ind <- sample(seq_len(nrow(data_select)), size_sample)
# training x test
data_train <- data_select[train_ind, ]
data_test <- data_select[-train_ind, ]
# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
test_task <- makeClassifTask(data = data_test, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
test_task <- normalizeFeatures(test_task, method = "standardize")
# removendo features
# train_task <- dropFeatures(task = train_task,features = c("grossura_pele"))
lda <- makeLearner("classif.lda", predict.type = "prob", method = "t", nu = 10)
getParamSet("classif.lda")
# gridsearch (parametros de tuning)
parameters <- makeParamSet("method", default = "mve"),
makeNumericLearnerParam("nu", lower = 2, upper = 40)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
lda_tune <- tuneParams(learner = lda, resampling = cv, task = train_task, par.set = parameters, control = control_tune)
# using hyperparameters
lda_tuning <- setHyperPars(lda, par.vals = lda_tune$x)
# configurando os parâmetros de tuning
lda <- mlr::train(lda_tuning, train_task)
getLearnerModel(rf)
# passando o conjunto de teste para o modelo
predict_lda<-predict(lda, test_task)
predict_rf
df <- data.frame(predict_lda$data)
# definindo a classe positiva como 1 (com diabetes)
# avaliar os resultados
library(caret)
confusionMatrix(df$response, df$truth, positive = levels(df$truth)[2])
# avaliando onde o modelo está errando
# df_error <- df[df$truth != df$response, ]
# df_error
#
# # selecionar as instâncias que o modelo errou
# data_select_error <- data_select[df_error$id,]
# data_select_error
beep()
roc.curve(df$truth, df$response, plotit = F)
# teste local do modelo
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
library(beepr)
library(mlr)
# tamanho dos dados para treino
size_sample <-  floor(0.7 * nrow(data_select))
set.seed(123)
# particionamento aleatório
train_ind <- sample(seq_len(nrow(data_select)), size_sample)
# training x test
data_train <- data_select[train_ind, ]
data_test <- data_select[-train_ind, ]
# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
test_task <- makeClassifTask(data = data_test, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
test_task <- normalizeFeatures(test_task, method = "standardize")
# removendo features
# train_task <- dropFeatures(task = train_task,features = c("grossura_pele"))
lda <- makeLearner("classif.lda", predict.type = "prob", method = "t", nu = 10)
getParamSet("classif.lda")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeNumericLearnerParam("nu", lower = 2, upper = 40)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
lda_tune <- tuneParams(learner = lda, resampling = cv, task = train_task, par.set = parameters, control = control_tune)
# using hyperparameters
lda_tuning <- setHyperPars(lda, par.vals = lda_tune$x)
# configurando os parâmetros de tuning
lda <- mlr::train(lda_tuning, train_task)
getLearnerModel(rf)
# passando o conjunto de teste para o modelo
predict_lda<-predict(lda, test_task)
predict_rf
df <- data.frame(predict_lda$data)
# definindo a classe positiva como 1 (com diabetes)
# avaliar os resultados
library(caret)
confusionMatrix(df$response, df$truth, positive = levels(df$truth)[2])
# avaliando onde o modelo está errando
# df_error <- df[df$truth != df$response, ]
# df_error
#
# # selecionar as instâncias que o modelo errou
# data_select_error <- data_select[df_error$id,]
# data_select_error
beep()
roc.curve(df$truth, df$response, plotit = F)
setwd("GitHub/predict_diabetes_diagnostic/")
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
# balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "both",N = 680, seed = 1)$data
# data_select <- balanced_target
############################# Preparando para a submissão ao Kaggle #############################
library(mlr)
set.seed(123)
# criando as tasks
train_task <- makeClassifTask(data = data_select, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
# removendo features
# train_task <- dropFeatures(task = train_task, features = c("grossura_pele"))
lda <- makeLearner("classif.lda", predict.type = "prob", method = "t", nu = 10)
getParamSet("classif.lda")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeNumericLearnerParam("nu", lower = 2, upper = 40)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
lda_tune <- tuneParams(learner = lda, resampling = cv, task = train_task, par.set = parameters, control = control_tune)
# using hyperparameters
lda_tuning <- setHyperPars(lda, par.vals = lda_tune$x)
# configurando os parâmetros de tuning
lda <- mlr::train(lda_tuning, train_task)
getLearnerModel(lda)
# realizar as predições no dataset de teste do kaggle
data_test <- read.csv("data/dataset_teste.csv", header = TRUE, sep = ",")
data_test <- normalizeFeatures(data_test, method = "standardize")
# passando o conjunto de teste para o modelo
predict_lda<-predict(lda, test_task)
predict_lda
# adequando para o formato do kaggle
df_predictions <- data.frame(id = cbind(1:length(predict_lda),as.data.frame(predict_lda)))
colnames(df_predictions) <- c("id", "classe")
head(df_predictions)
tail(df_predictions)
setwd("GitHub/predict_diabetes_diagnostic/")
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
# balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "both",N = 680, seed = 1)$data
# data_select <- balanced_target
############################# Preparando para a submissão ao Kaggle #############################
library(mlr)
set.seed(123)
# criando as tasks
train_task <- makeClassifTask(data = data_select, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
# removendo features
# train_task <- dropFeatures(task = train_task, features = c("grossura_pele"))
lda <- makeLearner("classif.lda", predict.type = "prob", method = "t", nu = 10)
getParamSet("classif.lda")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeNumericLearnerParam("nu", lower = 2, upper = 40)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
lda_tune <- tuneParams(learner = lda, resampling = cv, task = train_task, par.set = parameters, control = control_tune)
# using hyperparameters
lda_tuning <- setHyperPars(lda, par.vals = lda_tune$x)
# configurando os parâmetros de tuning
lda <- mlr::train(lda_tuning, train_task)
getLearnerModel(lda)
# realizar as predições no dataset de teste do kaggle
data_test <- read.csv("data/dataset_teste.csv", header = TRUE, sep = ",")
data_test <- normalizeFeatures(data_test, method = "standardize")
# passando o conjunto de teste para o modelo
predict_lda<-predict(lda, data_test)
predict_lda
# adequando para o formato do kaggle
df_predictions <- data.frame(id = cbind(1:length(predict_lda),as.data.frame(predict_lda)))
colnames(df_predictions) <- c("id", "classe")
head(df_predictions)
tail(df_predictions)
setwd("GitHub/predict_diabetes_diagnostic/")
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
# balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "both",N = 680, seed = 1)$data
# data_select <- balanced_target
############################# Preparando para a submissão ao Kaggle #############################
library(mlr)
set.seed(123)
# criando as tasks
train_task <- makeClassifTask(data = data_select, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
# removendo features
# train_task <- dropFeatures(task = train_task, features = c("grossura_pele"))
lda <- makeLearner("classif.lda", predict.type = "prob", method = "t", nu = 10)
getParamSet("classif.lda")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeNumericLearnerParam("nu", lower = 2, upper = 40)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
lda_tune <- tuneParams(learner = lda, resampling = cv, task = train_task, par.set = parameters, control = control_tune)
# using hyperparameters
lda_tuning <- setHyperPars(lda, par.vals = lda_tune$x)
# configurando os parâmetros de tuning
lda <- mlr::train(lda_tuning, train_task)
getLearnerModel(lda)
# realizar as predições no dataset de teste do kaggle
data_test <- read.csv("data/dataset_teste.csv", header = TRUE, sep = ",")
data_test <- normalizeFeatures(data_test, method = "standardize")
predict_lda<-predict(lda, data_test)
predict_lda
setwd("GitHub/predict_diabetes_diagnostic/")
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
# balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "both",N = 680, seed = 1)$data
# data_select <- balanced_target
############################# Preparando para a submissão ao Kaggle #############################
library(mlr)
set.seed(123)
# criando as tasks
train_task <- makeClassifTask(data = data_select, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
# removendo features
# train_task <- dropFeatures(task = train_task, features = c("grossura_pele"))
lda <- makeLearner("classif.lda", predict.type = "prob", method = "t", nu = 10)
getParamSet("classif.lda")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeNumericLearnerParam("nu", lower = 2, upper = 40)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
lda_tune <- tuneParams(learner = lda, resampling = cv, task = train_task, par.set = parameters, control = control_tune)
# using hyperparameters
lda_tuning <- setHyperPars(lda, par.vals = lda_tune$x)
# configurando os parâmetros de tuning
lda <- mlr::train(lda_tuning, train_task)
getLearnerModel(lda)
# realizar as predições no dataset de teste do kaggle
data_test <- read.csv("data/dataset_teste.csv", header = TRUE, sep = ",")
data_test <- normalizeFeatures(data_test, method = "standardize")
# passando o conjunto de teste para o modelo
predict_lda<-predict(lda$learner.model, data_test)
predict_lda
# adequando para o formato do kaggle
df_predictions <- data.frame(id = cbind(1:length(predict_lda),as.data.frame(predict_lda)))
colnames(df_predictions) <- c("id", "classe")
head(df_predictions)
tail(df_predictions)
predict_lda
# adequando para o formato do kaggle
df_predictions <- data.frame(id = cbind(1:length(predict_lda$class),as.data.frame(predict_lda$class)))
colnames(df_predictions) <- c("id", "classe")
head(df_predictions)
tail(df_predictions)
table(df_predictions$classe)
table(data_select$classe)
prop.table(df_predictions$classe)
prop.table(table(df_predictions$classe))
prop.table(table(data_select$classe))
predict_lda$class
predict_lda$posterior
head(predict_lda$x)
# adequando para o formato do kaggle
df_predictions <- data.frame(id = cbind(1:length(predict_lda$class),as.data.frame(predict_lda$class)))
colnames(df_predictions) <- c("id", "classe")
head(df_predictions)
tail(df_predictions)
table(df_predictions$classe)
# gravando a saida
write.csv(df_predictions,file="submissoes/df_predictions_7.csv", row.names=FALSE)
# teste local do modelo
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
library(beepr)
library(mlr)
# tamanho dos dados para treino
size_sample <-  floor(0.7 * nrow(data_select))
set.seed(123)
# particionamento aleatório
train_ind <- sample(seq_len(nrow(data_select)), size_sample)
# training x test
data_train <- data_select[train_ind, ]
data_test <- data_select[-train_ind, ]
# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
test_task <- makeClassifTask(data = data_test, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
test_task <- normalizeFeatures(test_task, method = "standardize")
# removendo features
# train_task <- dropFeatures(task = train_task,features = c("grossura_pele"))
lda <- makeLearner("classif.lda", predict.type = "prob", method = "t", nu = 10)
getParamSet("classif.lda")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeNumericLearnerParam("nu", lower = 2, upper = 40)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
lda_tune <- tuneParams(learner = lda, resampling = cv, task = train_task, par.set = parameters, control = control_tune)
# using hyperparameters
lda_tuning <- setHyperPars(lda, par.vals = lda_tune$x)
# configurando os parâmetros de tuning
lda <- mlr::train(lda_tuning, train_task)
getLearnerModel(rf)
# passando o conjunto de teste para o modelo
predict_lda<-predict(lda, test_task)
predict_rf
df <- data.frame(predict_lda$data)
# definindo a classe positiva como 1 (com diabetes)
# avaliar os resultados
library(caret)
confusionMatrix(df$response, df$truth, positive = levels(df$truth)[2])
# avaliando onde o modelo está errando
# df_error <- df[df$truth != df$response, ]
# df_error
#
# # selecionar as instâncias que o modelo errou
# data_select_error <- data_select[df_error$id,]
# data_select_error
beep()
roc.curve(df$truth, df$response, plotit = F)
predict_lda$data
predict_lda$data
?predict
predict_lda$predict.type
predict_lda$threshold
predict_lda$threshold[0]
as.numeric(predict_lda$threshold)
class(predict_lda$threshold)
data_select$classe
table(data_select$classe)
prop.table(table(data_select$classe))
predict_lda$threshold
predict_lda$threshold <- c(0.6, 0.4)
# teste local do modelo
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
library(beepr)
library(mlr)
# tamanho dos dados para treino
size_sample <-  floor(0.7 * nrow(data_select))
set.seed(123)
# particionamento aleatório
train_ind <- sample(seq_len(nrow(data_select)), size_sample)
# training x test
data_train <- data_select[train_ind, ]
data_test <- data_select[-train_ind, ]
# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
test_task <- makeClassifTask(data = data_test, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
test_task <- normalizeFeatures(test_task, method = "standardize")
# removendo features
# train_task <- dropFeatures(task = train_task,features = c("grossura_pele"))
lda <- makeLearner("classif.lda", predict.type = "prob", method = "t", nu = 10)
getParamSet("classif.lda")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeNumericLearnerParam("nu", lower = 2, upper = 40)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
lda_tune <- tuneParams(learner = lda, resampling = cv, task = train_task, par.set = parameters, control = control_tune)
# using hyperparameters
lda_tuning <- setHyperPars(lda, par.vals = lda_tune$x)
# configurando os parâmetros de tuning
lda <- mlr::train(lda_tuning, train_task)
getLearnerModel(lda)
# passando o conjunto de teste para o modelo
predict_lda<-predict(lda, test_task)
predict_lda$threshold <- c(0.6, 0.4)
predict_lda
df <- data.frame(predict_lda$data)
# definindo a classe positiva como 1 (com diabetes)
# avaliar os resultados
library(caret)
confusionMatrix(df$response, df$truth, positive = levels(df$truth)[2])
# avaliando onde o modelo está errando
# df_error <- df[df$truth != df$response, ]
# df_error
#
# # selecionar as instâncias que o modelo errou
# data_select_error <- data_select[df_error$id,]
# data_select_error
beep()
roc.curve(df$truth, df$response, plotit = F)
predict_lda
head(df)
prop.table(table(data_select$classe))
ifelse(df$id > .6, 1, 0)
ifelse(df$prob.0 > .4, 1, 0)
ifelse(df$prob.0 > .6, 1, 0)
table(df$response)
ifelse(df$prob.0 > .6, 1, 0)
table(ifelse(df$prob.0 > .6, 1, 0))
ifelse(df$prob.0 > .4, 1, 0)
table(ifelse(df$prob.0 > .4, 1, 0)
)
table(ifelse(df$prob.1 > .4, 1, 0))
df$prob.1
df
## controlando o threshold
table(ifelse(df$prob.1 > .4, 1, 0))
predict_lda
head(df)
# teste local do modelo
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
head(data_select)
library(beepr)
library(mlr)
# tamanho dos dados para treino
size_sample <-  floor(0.7 * nrow(data_select))
set.seed(123)
# particionamento aleatório
train_ind <- sample(seq_len(nrow(data_select)), size_sample)
# training x test
data_train <- data_select[train_ind, ]
data_test <- data_select[-train_ind, ]
# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
test_task <- makeClassifTask(data = data_test, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
test_task <- normalizeFeatures(test_task, method = "standardize")
# removendo features
# train_task <- dropFeatures(task = train_task,features = c("grossura_pele"))
lda <- makeLearner("classif.lda", predict.type = "prob", method = "t", nu = 10)
getParamSet("classif.lda")
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeNumericLearnerParam("nu", lower = 2, upper = 40)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
lda_tune <- tuneParams(learner = lda, resampling = cv, task = train_task, par.set = parameters, control = control_tune)
# using hyperparameters
lda_tuning <- setHyperPars(lda, par.vals = lda_tune$x)
# configurando os parâmetros de tuning
lda <- mlr::train(lda_tuning, train_task)
getLearnerModel(lda)
# passando o conjunto de teste para o modelo
predict_lda<-predict(lda, test_task)
predict_lda
df <- data.frame(predict_lda$data)
head(df)
## controlando o threshold
table(ifelse(df$prob.1 > .4, 1, 0))
# definindo a classe positiva como 1 (com diabetes)
# avaliar os resultados
library(caret)
confusionMatrix(df$response, df$truth, positive = levels(df$truth)[2])
# avaliando onde o modelo está errando
# df_error <- df[df$truth != df$response, ]
# df_error
#
# # selecionar as instâncias que o modelo errou
# data_select_error <- data_select[df_error$id,]
# data_select_error
beep()
roc.curve(df$truth, df$response, plotit = F)
