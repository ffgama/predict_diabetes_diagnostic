rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf, test_task)
df <- data.frame(predict_rf$data)
# avaliar os resultados
library(caret)
confusionMatrix(df$response, df$truth)
beep()
library(pROC)
auc(response = df$truth, predictor = factor(df$response, ordered = TRUE))
# teste local do modelo
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
library(beepr)
library(mlr)
# tamanho dos dados para treino
size_sample <-  floor(0.7 * nrow(data_select))
set.seed(123)
# particionamento aleatório
train_ind <- sample(seq_len(nrow(data_select)), size_sample)
# training x test
data_train <- data_select[train_ind, ]
data_test <- data_select[-train_ind, ]
# criando as tasks
train_task <- makeClassifTask(data = data_train, target = "classe")
test_task <- makeClassifTask(data = data_test, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
test_task <- normalizeFeatures(test_task, method = "standardize")
# removendo features
#train_task <- dropFeatures(task = train_task,features = c("pressao_sanguinea"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 200,
importance = TRUE,
cutoff = c(0.61,0.39)))
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L, stratify = TRUE)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf, test_task)
df <- data.frame(predict_rf$data)
# avaliar os resultados
library(caret)
confusionMatrix(df$response, df$truth)
beep()
library(pROC)
auc(response = df$truth, predictor = factor(df$response, ordered = TRUE))
setwd("GitHub/predict_diabetes_diagnostic/")
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
############################# Preparando para a submissão ao Kaggle #############################
library(mlr)
set.seed(123)
# criando as tasks
train_task <- makeClassifTask(data = data_select, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
# removendo features
#train_task <- dropFeatures(task = train_task,features = c("pressao_sanguinea"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 200,
importance = TRUE,
cutoff = c(0.61,0.39)))
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
setwd("GitHub/predict_diabetes_diagnostic/")
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
############################# Preparando para a submissão ao Kaggle #############################
library(mlr)
set.seed(123)
# criando as tasks
train_task <- makeClassifTask(data = data_select, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
# removendo features
train_task <- dropFeatures(task = train_task,features = c("pressao_sanguinea"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 200,
importance = TRUE,
cutoff = c(0.61,0.39)))
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
setwd("GitHub/predict_diabetes_diagnostic/")
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
############################# Preparando para a submissão ao Kaggle #############################
library(mlr)
set.seed(123)
# criando as tasks
train_task <- makeClassifTask(data = data_select, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
# removendo features
train_task <- dropFeatures(task = train_task,features = c("pressao_sanguinea"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 200,
importance = TRUE,
cutoff = c(0.61,0.39)))
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L, stratify = TRUE)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
# realizar as predições no dataset de teste do kaggle
data_test <- read.csv("data/dataset_teste.csv", header = TRUE, sep = ",")
data_test <- normalizeFeatures(data_test, method = "standardize")
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf$learner.model, data_test)
predict_rf
# adequando para o formato do kaggle
df_predictions <- data.frame(id = cbind(1:length(predict_rf),as.data.frame(predict_rf)))
colnames(df_predictions) <- c("id", "classe")
head(df_predictions)
tail(df_predictions)
table(df_predictions$classe)
setwd("GitHub/predict_diabetes_diagnostic/")
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
############################# Preparando para a submissão ao Kaggle #############################
library(mlr)
set.seed(123)
# criando as tasks
train_task <- makeClassifTask(data = data_select, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
# removendo features
train_task <- dropFeatures(task = train_task,features = c("pressao_sanguinea"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 200,
importance = TRUE,
cutoff = c(0.61,0.39)))
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
# realizar as predições no dataset de teste do kaggle
data_test <- read.csv("data/dataset_teste.csv", header = TRUE, sep = ",")
data_test <- normalizeFeatures(data_test, method = "standardize")
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf$learner.model, data_test)
predict_rf
# adequando para o formato do kaggle
df_predictions <- data.frame(id = cbind(1:length(predict_rf),as.data.frame(predict_rf)))
colnames(df_predictions) <- c("id", "classe")
head(df_predictions)
tail(df_predictions)
df_predictions$classe
table(df_predictions$classe)
# gravando a saida
write.csv(df_predictions,file="submissoes/df_predictions_11.csv", row.names=FALSE)
?str
# estrutura do dataset
str(open_data)
# Etapa de transformação
rm(list=ls())
# carga dos dados
load("data/load_data.RData")
# tamanho do dataset
dim(open_data)
# estrutura do dataset
str(open_data)
par(mfrow = c(2, 2))
# numero de gestacoes
box_gestacoes <- boxplot(open_data$num_gestacoes,  col = rgb(0.75,0.75,0.75))
title("Distribuição do número de gestações")
# concentracao de glicose
box_glicose <- boxplot(open_data$glicose,  col = rgb(0.75,0.75,0.75))
title("Distribuição da concentração de glicose")
open_data[open_data$glicose == 0,]$glicose <- NA
# pressao arterial  diastolica
box_pressao_sanguinea <- boxplot(open_data$pressao_sanguinea ,  col = rgb(0.75,0.75,0.75))
title("Distribuição da pressão arterial")
# pressão sanguinea igual a 0 naõ existe substituiremos pela mediana para não perder informação
open_data[open_data$pressao_sanguinea == 0,]$pressao_sanguinea <- NA
# espessura da dobra da pele
box_grossura_pele <- boxplot(open_data$grossura_pele ,   col = rgb(0.75,0.75,0.75))
title("Distribuição da grossura da pele")
open_data[open_data$grossura_pele == 0,]$grossura_pele <- NA
?knnImputation
getwd()
# Etapa de extração
# carregando o dataset de treino
open_data <- read.csv("data/dataset_treino.csv", header = TRUE, sep = ",")
head(open_data)
# Etapa de exploração
rm(list=ls())
# carga dos dados transformados
load("data/transforming_data.RData")
head(open_data_transf)
library(ggplot2)
# quem desenvolveu a doença normalmente um maior número de gestações
ggplot(open_data_transf, mapping = aes(x = classe, y = num_gestacoes, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# quem desenvolveu a doença normalmente apresenta um alto nível de glicose (entre 140 e 199 mg/DL)
ggplot(open_data_transf, mapping = aes(x = classe, y = glicose, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# existe pouca diferença entre a pressão sanguínea em pacientes com e sem diabetes.
ggplot(open_data_transf, mapping = aes(x = classe, y = pressao_sanguinea, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# a pele dos pacientes com diabetes é um pouco mais espessa. A grossura de pele em um paciente chegou próximo a 100 (seria um outlier?)
ggplot(open_data_transf, mapping = aes(x = classe, y = grossura_pele, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# pessoas com diabetes apresentam um valor de insulina elevado (acima de 200 especialmente). Em alguns casos de pessoas não diagnosticados o valor de
# insulina passou dos 200.
ggplot(open_data_transf, mapping = aes(x = classe, y = insulina, group = classe, fill = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# observar a distribuição através de um histograma
par(mfrow = c(2, 1))
hist(subset(open_data_transf, classe == 1)$insulina, labels = TRUE, col = "red", main = "Presença de diabetes")
hist(subset(open_data_transf, classe == 0)$insulina, labels = TRUE, col = "blue", main = "Ausência de diabetes")
# Etapa de exploração
rm(list=ls())
# carga dos dados transformados
load("data/transforming_data.RData")
head(open_data_transf)
library(ggplot2)
# quem desenvolveu a doença normalmente um maior número de gestações
ggplot(open_data_transf, mapping = aes(x = classe, y = num_gestacoes, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# quem desenvolveu a doença normalmente apresenta um alto nível de glicose (entre 140 e 199 mg/DL)
ggplot(open_data_transf, mapping = aes(x = classe, y = glicose, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# existe pouca diferença entre a pressão sanguínea em pacientes com e sem diabetes.
ggplot(open_data_transf, mapping = aes(x = classe, y = pressao_sanguinea, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# a pele dos pacientes com diabetes é um pouco mais espessa. A grossura de pele em um paciente chegou próximo a 100 (seria um outlier?)
ggplot(open_data_transf, mapping = aes(x = classe, y = grossura_pele, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# pessoas com diabetes apresentam um valor de insulina elevado (acima de 200 especialmente). Em alguns casos de pessoas não diagnosticados o valor de
# insulina passou dos 200.
ggplot(open_data_transf, mapping = aes(x = classe, y = insulina, group = classe, fill = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# Etapa de exploração
rm(list=ls())
# carga dos dados transformados
load("data/transforming_data.RData")
head(open_data_transf)
library(ggplot2)
# quem desenvolveu a doença normalmente um maior número de gestações
ggplot(open_data_transf, mapping = aes(x = classe, y = num_gestacoes, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# quem desenvolveu a doença normalmente apresenta um alto nível de glicose (entre 140 e 199 mg/DL)
ggplot(open_data_transf, mapping = aes(x = classe, y = glicose, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# existe pouca diferença entre a pressão sanguínea em pacientes com e sem diabetes.
ggplot(open_data_transf, mapping = aes(x = classe, y = pressao_sanguinea, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# a pele dos pacientes com diabetes é um pouco mais espessa. A grossura de pele em um paciente chegou próximo a 100 (seria um outlier?)
ggplot(open_data_transf, mapping = aes(x = classe, y = grossura_pele, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# pessoas com diabetes apresentam um valor de insulina elevado (acima de 200 especialmente). Em alguns casos de pessoas não diagnosticados o valor de
# insulina passou dos 200.
ggplot(open_data_transf, mapping = aes(x = classe, y = insulina, group = classe, fill = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# Etapa de exploração
rm(list=ls())
# carga dos dados transformados
load("data/transforming_data.RData")
head(open_data_transf)
library(ggplot2)
# quem desenvolveu a doença normalmente um maior número de gestações
ggplot(open_data_transf, mapping = aes(x = classe, y = num_gestacoes, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# quem desenvolveu a doença normalmente apresenta um alto nível de glicose (entre 140 e 199 mg/DL)
ggplot(open_data_transf, mapping = aes(x = classe, y = glicose, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# existe pouca diferença entre a pressão sanguínea em pacientes com e sem diabetes.
ggplot(open_data_transf, mapping = aes(x = classe, y = pressao_sanguinea, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# a pele dos pacientes com diabetes é um pouco mais espessa. A grossura de pele em um paciente chegou próximo a 100 (seria um outlier?)
ggplot(open_data_transf, mapping = aes(x = classe, y = grossura_pele, group = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
# pessoas com diabetes apresentam um valor de insulina elevado (acima de 200 especialmente). Em alguns casos de pessoas não diagnosticados o valor de
# insulina passou dos 200.
ggplot(open_data_transf, mapping = aes(x = classe, y = insulina, group = classe, fill = classe))+
geom_boxplot(fill=c("#57A0D3","#C90D06"))+
theme_minimal()
library(beepr)
library(mlr)
install.packages("beepr")
install.packages("mlr")
install.packages("caret")
install.packages("ROSE")
library(ROSE)
setwd("GitHub/predict_diabetes_diagnostic/")
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
library(ROSE)
balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "both",N = 680, seed = 1)$data
data_select <- balanced_target
install.packages("randomFrest")
install.packages("randomForest")
setwd("GitHub/predict_diabetes_diagnostic/")
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
library(ROSE)
balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "both",N = 680, seed = 1)$data
data_select <- balanced_target
############################# Preparando para a submissão ao Kaggle #############################
library(mlr)
set.seed(123)
# criando as tasks
train_task <- makeClassifTask(data = data_select, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
# removendo features
train_task <- dropFeatures(task = train_task,features = c("pressao_sanguinea"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 200,
importance = TRUE
))
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L, stratify = TRUE)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
# realizar as predições no dataset de teste do kaggle
data_test <- read.csv("data/dataset_teste.csv", header = TRUE, sep = ",")
data_test <- normalizeFeatures(data_test, method = "standardize")
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf$learner.model, data_test)
predict_rf
# adequando para o formato do kaggle
df_predictions <- data.frame(id = cbind(1:length(predict_rf),as.data.frame(predict_rf)))
colnames(df_predictions) <- c("id", "classe")
head(df_predictions)
tail(df_predictions)
# gravando a saida
write.csv(df_predictions,file="submissoes/df_predictions_late_sub_1.csv", row.names=FALSE)
table(df_predictions$classe)
setwd("GitHub/predict_diabetes_diagnostic/")
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
library(ROSE)
balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "both",N = 680, seed = 1)$data
data_select <- balanced_target
############################# Preparando para a submissão ao Kaggle #############################
library(mlr)
set.seed(123)
# criando as tasks
train_task <- makeClassifTask(data = data_select, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
# removendo features
#train_task <- dropFeatures(task = train_task,features = c("pressao_sanguinea"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 200,
importance = TRUE
))
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L, stratify = TRUE)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
# realizar as predições no dataset de teste do kaggle
data_test <- read.csv("data/dataset_teste.csv", header = TRUE, sep = ",")
data_test <- normalizeFeatures(data_test, method = "standardize")
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf$learner.model, data_test)
predict_rf
# adequando para o formato do kaggle
df_predictions <- data.frame(id = cbind(1:length(predict_rf),as.data.frame(predict_rf)))
colnames(df_predictions) <- c("id", "classe")
head(df_predictions)
tail(df_predictions)
# gravando a saida
write.csv(df_predictions,file="submissoes/df_predictions_late_sub_1.csv", row.names=FALSE)
setwd("GitHub/predict_diabetes_diagnostic/")
# Etapa de criação do modelo
rm(list=ls())
# carga dos dados transformados
load("data/features_selection.RData")
library(ROSE)
balanced_target <- ovun.sample(classe ~ ., data = data_select, method = "both",N = 680, seed = 1)$data
data_select <- balanced_target
############################# Preparando para a submissão ao Kaggle #############################
library(mlr)
set.seed(123)
# criando as tasks
train_task <- makeClassifTask(data = data_select, target = "classe")
# # normalizando as variáveis
train_task <- normalizeFeatures(train_task, method = "standardize" )
# removendo features
#train_task <- dropFeatures(task = train_task,features = c("pressao_sanguinea"))
rf <- makeLearner("classif.randomForest", predict.type = "response", par.vals = list(mtry = 3, ntree = 200,
importance = TRUE
))
# gridsearch (parametros de tuning)
parameters <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 8),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
control_tune <- makeTuneControlRandom(maxit = 100L)
# cv
cv <- makeResampleDesc("CV",iters = 10L)
rf_tune <- tuneParams(learner = rf, resampling = cv, task = train_task, par.set = parameters, control = control_tune, show.info = TRUE)
# using hyperparameters
rf_tuning <- setHyperPars(rf, par.vals = rf_tune$x)
# configurando os parâmetros de tuning
rf <- mlr::train(rf_tuning, train_task)
# realizar as predições no dataset de teste do kaggle
data_test <- read.csv("data/dataset_teste.csv", header = TRUE, sep = ",")
data_test <- normalizeFeatures(data_test, method = "standardize")
# passando o conjunto de teste para o modelo
predict_rf<-predict(rf$learner.model, data_test)
predict_rf
# adequando para o formato do kaggle
df_predictions <- data.frame(id = cbind(1:length(predict_rf),as.data.frame(predict_rf)))
colnames(df_predictions) <- c("id", "classe")
head(df_predictions)
tail(df_predictions)
# gravando a saida
write.csv(df_predictions,file="submissoes/df_predictions_late_sub_1.csv", row.names=FALSE)
rm(list=ls())
